{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание\n",
    "\n",
    "## Данные о студенте\n",
    "\n",
    "1. **ФИО**: Денисов Даниил Михайлович\n",
    "2. **Факультет**: Механико-математический\n",
    "3. **Курс**: 2 (магистратура)\n",
    "4. **Группа**: М2\n",
    "\n",
    "## Замечания\n",
    "\n",
    "* Заполненный ноутбук необходимо сдать боту\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание состоит из трёх частей:\n",
    "1. Реализация прямого вывода нейронной сети (5 баллов)\n",
    "2. Реализация градиентов по входу и распространения градиента по сети (5 баллов)\n",
    "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети (10 баллов)\n",
    "\n",
    "Дополнительные баллы можно получить при реализации обучения сети со свёрточными слоями (10 баллов), с транспонированной свёрткой (10 баллов), дополнительного оптимизатора (5 баллов). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Реализация вывода собственной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
    "- конструктор\n",
    "- прямой вывод \n",
    "- обратный вывод, производные по входу и по параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Layer'\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "\n",
    "    def backward(self, input_data):\n",
    "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        pass\n",
    "\n",
    "    def grad_param(self, input_data):\n",
    "        return []\n",
    "    \n",
    "    def update_param(self, grads, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = 'Network'\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "    \n",
    "    def grad_x(self, input_data, labels):\n",
    "        # Intermediary gradients (forward pass)\n",
    "        grads_inter = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            grads_inter.append(layer.grad_x(current_input))\n",
    "            current_input = layer.forward(current_input)\n",
    "\n",
    "        # Target gradient (backward pass)\n",
    "        grad = self.loss.grad_x(current_input, labels)\n",
    "        for layer_grad in reversed(grads_inter):\n",
    "            grad = np.einsum('bi,bij->bj', grad, layer_grad)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def grad_param(self, input_data, labels):\n",
    "        # Intermediary gradients (forward pass)\n",
    "        grads_inter = []\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            grads_inter.append(layer.backward(current_input))\n",
    "            current_input = layer.forward(current_input)\n",
    "        \n",
    "        # Target gradients (backward pass)\n",
    "        grads_targ = []\n",
    "        grad = self.loss.grad_x(current_input, labels)\n",
    "        for layer_grad_x, layer_grad_param in reversed(grads_inter):\n",
    "            grads_targ.append([np.einsum('bi,bij->bj', grad, grad_param) \n",
    "                               for grad_param in layer_grad_param])\n",
    "            grad = np.einsum('bi,bij->bj', grad, layer_grad_x)\n",
    "\n",
    "        return grads_targ[::-1]\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for layer, grad in zip(self.layers, grads):\n",
    "            layer.update_param(grad, learning_rate)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "    \n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "    \n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grads = self.grad_param(input_data, labels)\n",
    "        self.update(grads, learning_rate)\n",
    "    \n",
    "    def fit(self, trainX, trainY, validation_split=0.25, \n",
    "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
    "        \n",
    "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=42)\n",
    "        \n",
    "        for epoch in range(nb_epoch):\n",
    "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
    "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            \n",
    "            print('%d epoch: val %.2f' %(epoch + 1, self.evaluate(val_x, val_y)))\n",
    "            \n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)\n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        return np.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
    "\n",
    "- DenseLayer\n",
    "- ReLU\n",
    "- Softmax\n",
    "- FlattenLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "        self.name = 'Dense'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Use LeCun initialization by default\n",
    "        self.W = W_init if W_init is not None \\\n",
    "                 else np.sqrt(3 / input_dim) * (2 * np.random.random((input_dim, output_dim)) - 1)\n",
    "        self.b = b_init if b_init is not None \\\n",
    "                 else np.zeros(output_dim, 'float32')\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return np.einsum('bi,ij->bj', input_data, self.W) + self.b\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        # dy/dx = W^T\n",
    "        return np.tile(self.W.T, reps=(len(input_data), 1, 1))\n",
    "\n",
    "    def grad_W(self, input_data):\n",
    "        # dy/dW = (x_1 * I ... x_n * I)\n",
    "        W_rows, W_cols = self.W.shape\n",
    "        grad = np.zeros((len(input_data), W_rows, W_cols, W_cols))\n",
    "        diag = np.einsum('bijj->bij', grad)\n",
    "        diag[:] = input_data[..., None]\n",
    "        return grad.transpose(0, 2, 1, 3).reshape(len(input_data), W_cols, -1)\n",
    "\n",
    "    def grad_b(self, input_data):\n",
    "        # dy/db = I\n",
    "        return np.tile(np.eye(len(self.b)), reps=(len(input_data), 1, 1))\n",
    "\n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "    \n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "    \n",
    "    def update_b(self, grad, learning_rate):\n",
    "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return np.clip(input_data, 0, None)\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        batch, *dims = input_data.shape\n",
    "        size = np.prod(dims)\n",
    "        grad = np.zeros((batch, size, size))\n",
    "        diag = np.einsum('bii->bi', grad)\n",
    "        diag[:] = np.ceil(np.clip(input_data, 0, 1)).reshape(batch, -1)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Softmax'\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        exps = np.e ** input_data\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        # dy/dx = diag(e^x/S) - (e^x/S)^T * (e^x/S)\n",
    "        forward = self.forward(input_data)\n",
    "        grad = -np.einsum('bi,bj->bij', forward, forward)\n",
    "        diag = np.einsum('bii->bi', grad)\n",
    "        diag[:] = forward + diag\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Flatten'\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        return input_data.reshape(len(input_data), -1)\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        batch, *dims = input_data.shape\n",
    "        size = np.prod(dims)\n",
    "        return np.tile(np.eye(size), reps=(batch, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Реализуйте теперь свёрточный слой и транспонированную свёртку  (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, c_in, c_out, h_in, w_in = 1, 2, 2, 4, 4\n",
    "input_data = np.arange(b*c_in*h_in*w_in).reshape((b, c_in, h_in, w_in)).astype('float32')\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_l, pad_r = 0, 0\n",
    "input_data = np.pad(input_data, ((0, 0), (0, 0), (pad_l, pad_r), (pad_l, pad_r)))\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = pad_l + pad_r\n",
    "h_in += pad\n",
    "w_in += pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, s = 3, 1\n",
    "kernel = np.tile(np.eye(k), reps=(c_in, 1, 1)) * np.array([[[1]], [[2]]])\n",
    "kernel = np.tile(kernel, reps=(c_out, 1, 1, 1))\n",
    "kernel = kernel.transpose(2, 3, 1, 0)\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_out, w_out = (h_in - k) // s + 1, (w_in - k) // s + 1\n",
    "h_out, w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [1],\n",
       "        [2]],\n",
       "\n",
       "       [[0],\n",
       "        [1],\n",
       "        [2]],\n",
       "\n",
       "       [[1],\n",
       "        [2],\n",
       "        [3]],\n",
       "\n",
       "       [[1],\n",
       "        [2],\n",
       "        [3]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idh_in = np.arange(k) + np.expand_dims(s * np.arange(h_out), axis=1).repeat(k, axis=1)\n",
    "idh_in = np.expand_dims(idh_in.repeat(w_out, axis=0), axis=2)\n",
    "idh_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2]],\n",
       "\n",
       "       [[1, 2, 3]],\n",
       "\n",
       "       [[0, 1, 2]],\n",
       "\n",
       "       [[1, 2, 3]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idw_in = np.arange(k) + (s * np.arange(w_out))[:, None].repeat(k, axis=1)\n",
    "idw_in = np.expand_dims(np.tile(idw_in, reps=(h_out, 1)), axis=1)\n",
    "idw_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0.,  1.,  2.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 8.,  9., 10.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.],\n",
       "          [ 5.,  6.,  7.],\n",
       "          [ 9., 10., 11.]],\n",
       "\n",
       "         [[ 4.,  5.,  6.],\n",
       "          [ 8.,  9., 10.],\n",
       "          [12., 13., 14.]],\n",
       "\n",
       "         [[ 5.,  6.,  7.],\n",
       "          [ 9., 10., 11.],\n",
       "          [13., 14., 15.]]],\n",
       "\n",
       "\n",
       "        [[[16., 17., 18.],\n",
       "          [20., 21., 22.],\n",
       "          [24., 25., 26.]],\n",
       "\n",
       "         [[17., 18., 19.],\n",
       "          [21., 22., 23.],\n",
       "          [25., 26., 27.]],\n",
       "\n",
       "         [[20., 21., 22.],\n",
       "          [24., 25., 26.],\n",
       "          [28., 29., 30.]],\n",
       "\n",
       "         [[21., 22., 23.],\n",
       "          [25., 26., 27.],\n",
       "          [29., 30., 31.]]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = input_data[:, :, idh_in, idw_in]\n",
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[142., 151.],\n",
       "         [178., 187.]],\n",
       "\n",
       "        [[142., 151.],\n",
       "         [178., 187.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = np.array([1])\n",
    "conv = np.tensordot(patches, kernel, axes=([1, 3, 4], [2, 0, 1]))\n",
    "conv = conv.transpose(0, 2, 1).reshape(b, c_out, h_out, w_out) + np.expand_dims(bias, axis=(0, 2, 3))\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[  0,   1,   2,   3],\n",
       "            [  4,   5,   6,   7],\n",
       "            [  8,   9,  10,  11],\n",
       "            [ 12,  13,  14,  15]],\n",
       "\n",
       "           [[ 16,  17,  18,  19],\n",
       "            [ 20,  21,  22,  23],\n",
       "            [ 24,  25,  26,  27],\n",
       "            [ 28,  29,  30,  31]]],\n",
       "\n",
       "\n",
       "          [[[ 32,  33,  34,  35],\n",
       "            [ 36,  37,  38,  39],\n",
       "            [ 40,  41,  42,  43],\n",
       "            [ 44,  45,  46,  47]],\n",
       "\n",
       "           [[ 48,  49,  50,  51],\n",
       "            [ 52,  53,  54,  55],\n",
       "            [ 56,  57,  58,  59],\n",
       "            [ 60,  61,  62,  63]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 64,  65,  66,  67],\n",
       "            [ 68,  69,  70,  71],\n",
       "            [ 72,  73,  74,  75],\n",
       "            [ 76,  77,  78,  79]],\n",
       "\n",
       "           [[ 80,  81,  82,  83],\n",
       "            [ 84,  85,  86,  87],\n",
       "            [ 88,  89,  90,  91],\n",
       "            [ 92,  93,  94,  95]]],\n",
       "\n",
       "\n",
       "          [[[ 96,  97,  98,  99],\n",
       "            [100, 101, 102, 103],\n",
       "            [104, 105, 106, 107],\n",
       "            [108, 109, 110, 111]],\n",
       "\n",
       "           [[112, 113, 114, 115],\n",
       "            [116, 117, 118, 119],\n",
       "            [120, 121, 122, 123],\n",
       "            [124, 125, 126, 127]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[128, 129, 130, 131],\n",
       "            [132, 133, 134, 135],\n",
       "            [136, 137, 138, 139],\n",
       "            [140, 141, 142, 143]],\n",
       "\n",
       "           [[144, 145, 146, 147],\n",
       "            [148, 149, 150, 151],\n",
       "            [152, 153, 154, 155],\n",
       "            [156, 157, 158, 159]]],\n",
       "\n",
       "\n",
       "          [[[160, 161, 162, 163],\n",
       "            [164, 165, 166, 167],\n",
       "            [168, 169, 170, 171],\n",
       "            [172, 173, 174, 175]],\n",
       "\n",
       "           [[176, 177, 178, 179],\n",
       "            [180, 181, 182, 183],\n",
       "            [184, 185, 186, 187],\n",
       "            [188, 189, 190, 191]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[192, 193, 194, 195],\n",
       "            [196, 197, 198, 199],\n",
       "            [200, 201, 202, 203],\n",
       "            [204, 205, 206, 207]],\n",
       "\n",
       "           [[208, 209, 210, 211],\n",
       "            [212, 213, 214, 215],\n",
       "            [216, 217, 218, 219],\n",
       "            [220, 221, 222, 223]]],\n",
       "\n",
       "\n",
       "          [[[224, 225, 226, 227],\n",
       "            [228, 229, 230, 231],\n",
       "            [232, 233, 234, 235],\n",
       "            [236, 237, 238, 239]],\n",
       "\n",
       "           [[240, 241, 242, 243],\n",
       "            [244, 245, 246, 247],\n",
       "            [248, 249, 250, 251],\n",
       "            [252, 253, 254, 255]]]]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_in -= pad\n",
    "w_in -= pad\n",
    "grad = np.arange(b*c_out*h_out*w_out*c_in*h_in*w_in).reshape((b, c_out, h_out, w_out, c_in, h_in, w_in))\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[  0,   1,   2,   3],\n",
       "            [  4,   5,   6,   7],\n",
       "            [  8,   9,  10,  11],\n",
       "            [ 12,  13,  14,  15]],\n",
       "\n",
       "           [[ 16,  17,  18,  19],\n",
       "            [ 20,  21,  22,  23],\n",
       "            [ 24,  25,  26,  27],\n",
       "            [ 28,  29,  30,  31]]],\n",
       "\n",
       "\n",
       "          [[[ 32,  33,  34,  35],\n",
       "            [ 36,  37,  38,  39],\n",
       "            [ 40,  41,  42,  43],\n",
       "            [ 44,  45,  46,  47]],\n",
       "\n",
       "           [[ 48,  49,  50,  51],\n",
       "            [ 52,  53,  54,  55],\n",
       "            [ 56,  57,  58,  59],\n",
       "            [ 60,  61,  62,  63]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 64,  65,  66,  67],\n",
       "            [ 68,  69,  70,  71],\n",
       "            [ 72,  73,  74,  75],\n",
       "            [ 76,  77,  78,  79]],\n",
       "\n",
       "           [[ 80,  81,  82,  83],\n",
       "            [ 84,  85,  86,  87],\n",
       "            [ 88,  89,  90,  91],\n",
       "            [ 92,  93,  94,  95]]],\n",
       "\n",
       "\n",
       "          [[[ 96,  97,  98,  99],\n",
       "            [100, 101, 102, 103],\n",
       "            [104, 105, 106, 107],\n",
       "            [108, 109, 110, 111]],\n",
       "\n",
       "           [[112, 113, 114, 115],\n",
       "            [116, 117, 118, 119],\n",
       "            [120, 121, 122, 123],\n",
       "            [124, 125, 126, 127]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[128, 129, 130, 131],\n",
       "            [132, 133, 134, 135],\n",
       "            [136, 137, 138, 139],\n",
       "            [140, 141, 142, 143]],\n",
       "\n",
       "           [[144, 145, 146, 147],\n",
       "            [148, 149, 150, 151],\n",
       "            [152, 153, 154, 155],\n",
       "            [156, 157, 158, 159]]],\n",
       "\n",
       "\n",
       "          [[[160, 161, 162, 163],\n",
       "            [164, 165, 166, 167],\n",
       "            [168, 169, 170, 171],\n",
       "            [172, 173, 174, 175]],\n",
       "\n",
       "           [[176, 177, 178, 179],\n",
       "            [180, 181, 182, 183],\n",
       "            [184, 185, 186, 187],\n",
       "            [188, 189, 190, 191]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[192, 193, 194, 195],\n",
       "            [196, 197, 198, 199],\n",
       "            [200, 201, 202, 203],\n",
       "            [204, 205, 206, 207]],\n",
       "\n",
       "           [[208, 209, 210, 211],\n",
       "            [212, 213, 214, 215],\n",
       "            [216, 217, 218, 219],\n",
       "            [220, 221, 222, 223]]],\n",
       "\n",
       "\n",
       "          [[[224, 225, 226, 227],\n",
       "            [228, 229, 230, 231],\n",
       "            [232, 233, 234, 235],\n",
       "            [236, 237, 238, 239]],\n",
       "\n",
       "           [[240, 241, 242, 243],\n",
       "            [244, 245, 246, 247],\n",
       "            [248, 249, 250, 251],\n",
       "            [252, 253, 254, 255]]]]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_in += pad\n",
    "w_in += pad\n",
    "grad = np.pad(grad, ((0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (pad_l, pad_r), (pad_l, pad_r)))\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1, 1, 1, 1),\n",
       " (1, 2, 1, 1, 1),\n",
       " (1, 1, 2, 1, 1),\n",
       " (2, 1, 1, 3, 1),\n",
       " (1, 2, 1, 1, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idh_out = np.expand_dims(np.arange(h_out), axis=(1, 2, 3, 4))\n",
    "idw_out = np.expand_dims(np.arange(w_out), axis=(0, 2, 3, 4))\n",
    "idc_in = np.expand_dims(np.arange(c_in), axis=(0, 1, 3, 4))\n",
    "\n",
    "idh_in = np.arange(k) + np.expand_dims(s * np.arange(h_out), axis=1).repeat(k, axis=1)\n",
    "idh_in = np.expand_dims(idh_in, axis=(1, 2, 4))\n",
    "\n",
    "idw_in = np.arange(k) + np.expand_dims(s * np.arange(w_out), axis=1).repeat(k, axis=1)\n",
    "idw_in = np.expand_dims(idw_in, axis=(0, 2, 3))\n",
    "\n",
    "idh_out.shape, idw_out.shape, idc_in.shape, idh_in.shape, idw_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1, 1, 2, 3, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(kernel.transpose(3, 2, 0, 1), axis=(0, 2, 3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[  0,   1,   2],\n",
       "            [  4,   5,   6],\n",
       "            [  8,   9,  10]],\n",
       "\n",
       "           [[ 16,  17,  18],\n",
       "            [ 20,  21,  22],\n",
       "            [ 24,  25,  26]]],\n",
       "\n",
       "\n",
       "          [[[ 33,  34,  35],\n",
       "            [ 37,  38,  39],\n",
       "            [ 41,  42,  43]],\n",
       "\n",
       "           [[ 49,  50,  51],\n",
       "            [ 53,  54,  55],\n",
       "            [ 57,  58,  59]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 68,  69,  70],\n",
       "            [ 72,  73,  74],\n",
       "            [ 76,  77,  78]],\n",
       "\n",
       "           [[ 84,  85,  86],\n",
       "            [ 88,  89,  90],\n",
       "            [ 92,  93,  94]]],\n",
       "\n",
       "\n",
       "          [[[101, 102, 103],\n",
       "            [105, 106, 107],\n",
       "            [109, 110, 111]],\n",
       "\n",
       "           [[117, 118, 119],\n",
       "            [121, 122, 123],\n",
       "            [125, 126, 127]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[128, 129, 130],\n",
       "            [132, 133, 134],\n",
       "            [136, 137, 138]],\n",
       "\n",
       "           [[144, 145, 146],\n",
       "            [148, 149, 150],\n",
       "            [152, 153, 154]]],\n",
       "\n",
       "\n",
       "          [[[161, 162, 163],\n",
       "            [165, 166, 167],\n",
       "            [169, 170, 171]],\n",
       "\n",
       "           [[177, 178, 179],\n",
       "            [181, 182, 183],\n",
       "            [185, 186, 187]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[196, 197, 198],\n",
       "            [200, 201, 202],\n",
       "            [204, 205, 206]],\n",
       "\n",
       "           [[212, 213, 214],\n",
       "            [216, 217, 218],\n",
       "            [220, 221, 222]]],\n",
       "\n",
       "\n",
       "          [[[229, 230, 231],\n",
       "            [233, 234, 235],\n",
       "            [237, 238, 239]],\n",
       "\n",
       "           [[245, 246, 247],\n",
       "            [249, 250, 251],\n",
       "            [253, 254, 255]]]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[:, :, idh_out, idw_out, idc_in, idh_in, idw_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[1., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 0.]],\n",
       "\n",
       "           [[2., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 1.],\n",
       "            [0., 0., 0., 0.]],\n",
       "\n",
       "           [[0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 2.],\n",
       "            [0., 0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0., 0., 0., 0.],\n",
       "            [1., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.]],\n",
       "\n",
       "           [[0., 0., 0., 0.],\n",
       "            [2., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 1.]],\n",
       "\n",
       "           [[0., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 2.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 0.]],\n",
       "\n",
       "           [[2., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 1.],\n",
       "            [0., 0., 0., 0.]],\n",
       "\n",
       "           [[0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 2.],\n",
       "            [0., 0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0., 0., 0., 0.],\n",
       "            [1., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.]],\n",
       "\n",
       "           [[0., 0., 0., 0.],\n",
       "            [2., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0., 0.],\n",
       "            [0., 1., 0., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 1.]],\n",
       "\n",
       "           [[0., 0., 0., 0.],\n",
       "            [0., 2., 0., 0.],\n",
       "            [0., 0., 2., 0.],\n",
       "            [0., 0., 0., 2.]]]]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_copy = np.zeros(grad.shape)\n",
    "grad_copy[:, :, idh_out, idw_out, idc_in, idh_in, idw_in] = np.expand_dims(kernel.transpose(3, 2, 0, 1), axis=(0, 2, 3))\n",
    "grad_copy[:, :, :, :, :, pad_l:h_in-pad_r, pad_l:w_in-pad_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_copy.reshape(b, c_out*h_out*w_out, c_in*h_in*w_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0.,  1.,  2.],\n",
       "          [ 4.,  5.,  6.],\n",
       "          [ 8.,  9., 10.]],\n",
       "\n",
       "         [[16., 17., 18.],\n",
       "          [20., 21., 22.],\n",
       "          [24., 25., 26.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 5.,  6.,  7.],\n",
       "          [ 9., 10., 11.]],\n",
       "\n",
       "         [[17., 18., 19.],\n",
       "          [21., 22., 23.],\n",
       "          [25., 26., 27.]]],\n",
       "\n",
       "\n",
       "        [[[ 4.,  5.,  6.],\n",
       "          [ 8.,  9., 10.],\n",
       "          [12., 13., 14.]],\n",
       "\n",
       "         [[20., 21., 22.],\n",
       "          [24., 25., 26.],\n",
       "          [28., 29., 30.]]],\n",
       "\n",
       "\n",
       "        [[[ 5.,  6.,  7.],\n",
       "          [ 9., 10., 11.],\n",
       "          [13., 14., 15.]],\n",
       "\n",
       "         [[21., 22., 23.],\n",
       "          [25., 26., 27.],\n",
       "          [29., 30., 31.]]]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_tr = patches.transpose(0, 2, 1, 3, 4)\n",
    "patches_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "          [[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]],\n",
       "\n",
       "           [[0., 0., 0.],\n",
       "            [0., 0., 0.],\n",
       "            [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "          [[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]],\n",
       "\n",
       "           [[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [1., 1., 1.]]]]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.eye(c_out)\n",
    "ind = np.expand_dims(ind, axis=(0, 2, 4, 5, 6))\n",
    "ind = np.tile(ind, reps=(b, 1, h_out*w_out, 1, c_in, k, k))\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2, 4, 2, 2, 3, 3), (1, 1, 4, 1, 2, 3, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_tr = np.expand_dims(patches_tr, axis=(1, 3))\n",
    "ind.shape, patches_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[ 0.,  1.,  2.],\n",
       "            [ 4.,  5.,  6.],\n",
       "            [ 8.,  9., 10.]],\n",
       "\n",
       "           [[16., 17., 18.],\n",
       "            [20., 21., 22.],\n",
       "            [24., 25., 26.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 1.,  2.,  3.],\n",
       "            [ 5.,  6.,  7.],\n",
       "            [ 9., 10., 11.]],\n",
       "\n",
       "           [[17., 18., 19.],\n",
       "            [21., 22., 23.],\n",
       "            [25., 26., 27.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 4.,  5.,  6.],\n",
       "            [ 8.,  9., 10.],\n",
       "            [12., 13., 14.]],\n",
       "\n",
       "           [[20., 21., 22.],\n",
       "            [24., 25., 26.],\n",
       "            [28., 29., 30.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 5.,  6.,  7.],\n",
       "            [ 9., 10., 11.],\n",
       "            [13., 14., 15.]],\n",
       "\n",
       "           [[21., 22., 23.],\n",
       "            [25., 26., 27.],\n",
       "            [29., 30., 31.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  1.,  2.],\n",
       "            [ 4.,  5.,  6.],\n",
       "            [ 8.,  9., 10.]],\n",
       "\n",
       "           [[16., 17., 18.],\n",
       "            [20., 21., 22.],\n",
       "            [24., 25., 26.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 1.,  2.,  3.],\n",
       "            [ 5.,  6.,  7.],\n",
       "            [ 9., 10., 11.]],\n",
       "\n",
       "           [[17., 18., 19.],\n",
       "            [21., 22., 23.],\n",
       "            [25., 26., 27.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 4.,  5.,  6.],\n",
       "            [ 8.,  9., 10.],\n",
       "            [12., 13., 14.]],\n",
       "\n",
       "           [[20., 21., 22.],\n",
       "            [24., 25., 26.],\n",
       "            [28., 29., 30.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]],\n",
       "\n",
       "           [[ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.],\n",
       "            [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 5.,  6.,  7.],\n",
       "            [ 9., 10., 11.],\n",
       "            [13., 14., 15.]],\n",
       "\n",
       "           [[21., 22., 23.],\n",
       "            [25., 26., 27.],\n",
       "            [29., 30., 31.]]]]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_tr = patches_tr * ind\n",
    "patches_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[ 0.,  0.],\n",
       "            [16.,  0.]],\n",
       "\n",
       "           [[ 1.,  0.],\n",
       "            [17.,  0.]],\n",
       "\n",
       "           [[ 2.,  0.],\n",
       "            [18.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 4.,  0.],\n",
       "            [20.,  0.]],\n",
       "\n",
       "           [[ 5.,  0.],\n",
       "            [21.,  0.]],\n",
       "\n",
       "           [[ 6.,  0.],\n",
       "            [22.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 8.,  0.],\n",
       "            [24.,  0.]],\n",
       "\n",
       "           [[ 9.,  0.],\n",
       "            [25.,  0.]],\n",
       "\n",
       "           [[10.,  0.],\n",
       "            [26.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 1.,  0.],\n",
       "            [17.,  0.]],\n",
       "\n",
       "           [[ 2.,  0.],\n",
       "            [18.,  0.]],\n",
       "\n",
       "           [[ 3.,  0.],\n",
       "            [19.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 5.,  0.],\n",
       "            [21.,  0.]],\n",
       "\n",
       "           [[ 6.,  0.],\n",
       "            [22.,  0.]],\n",
       "\n",
       "           [[ 7.,  0.],\n",
       "            [23.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 9.,  0.],\n",
       "            [25.,  0.]],\n",
       "\n",
       "           [[10.,  0.],\n",
       "            [26.,  0.]],\n",
       "\n",
       "           [[11.,  0.],\n",
       "            [27.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 4.,  0.],\n",
       "            [20.,  0.]],\n",
       "\n",
       "           [[ 5.,  0.],\n",
       "            [21.,  0.]],\n",
       "\n",
       "           [[ 6.,  0.],\n",
       "            [22.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 8.,  0.],\n",
       "            [24.,  0.]],\n",
       "\n",
       "           [[ 9.,  0.],\n",
       "            [25.,  0.]],\n",
       "\n",
       "           [[10.,  0.],\n",
       "            [26.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[12.,  0.],\n",
       "            [28.,  0.]],\n",
       "\n",
       "           [[13.,  0.],\n",
       "            [29.,  0.]],\n",
       "\n",
       "           [[14.,  0.],\n",
       "            [30.,  0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 5.,  0.],\n",
       "            [21.,  0.]],\n",
       "\n",
       "           [[ 6.,  0.],\n",
       "            [22.,  0.]],\n",
       "\n",
       "           [[ 7.,  0.],\n",
       "            [23.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[ 9.,  0.],\n",
       "            [25.,  0.]],\n",
       "\n",
       "           [[10.,  0.],\n",
       "            [26.,  0.]],\n",
       "\n",
       "           [[11.,  0.],\n",
       "            [27.,  0.]]],\n",
       "\n",
       "\n",
       "          [[[13.,  0.],\n",
       "            [29.,  0.]],\n",
       "\n",
       "           [[14.,  0.],\n",
       "            [30.,  0.]],\n",
       "\n",
       "           [[15.,  0.],\n",
       "            [31.,  0.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 0.,  0.],\n",
       "            [ 0., 16.]],\n",
       "\n",
       "           [[ 0.,  1.],\n",
       "            [ 0., 17.]],\n",
       "\n",
       "           [[ 0.,  2.],\n",
       "            [ 0., 18.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  4.],\n",
       "            [ 0., 20.]],\n",
       "\n",
       "           [[ 0.,  5.],\n",
       "            [ 0., 21.]],\n",
       "\n",
       "           [[ 0.,  6.],\n",
       "            [ 0., 22.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  8.],\n",
       "            [ 0., 24.]],\n",
       "\n",
       "           [[ 0.,  9.],\n",
       "            [ 0., 25.]],\n",
       "\n",
       "           [[ 0., 10.],\n",
       "            [ 0., 26.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  1.],\n",
       "            [ 0., 17.]],\n",
       "\n",
       "           [[ 0.,  2.],\n",
       "            [ 0., 18.]],\n",
       "\n",
       "           [[ 0.,  3.],\n",
       "            [ 0., 19.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  5.],\n",
       "            [ 0., 21.]],\n",
       "\n",
       "           [[ 0.,  6.],\n",
       "            [ 0., 22.]],\n",
       "\n",
       "           [[ 0.,  7.],\n",
       "            [ 0., 23.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  9.],\n",
       "            [ 0., 25.]],\n",
       "\n",
       "           [[ 0., 10.],\n",
       "            [ 0., 26.]],\n",
       "\n",
       "           [[ 0., 11.],\n",
       "            [ 0., 27.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  4.],\n",
       "            [ 0., 20.]],\n",
       "\n",
       "           [[ 0.,  5.],\n",
       "            [ 0., 21.]],\n",
       "\n",
       "           [[ 0.,  6.],\n",
       "            [ 0., 22.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  8.],\n",
       "            [ 0., 24.]],\n",
       "\n",
       "           [[ 0.,  9.],\n",
       "            [ 0., 25.]],\n",
       "\n",
       "           [[ 0., 10.],\n",
       "            [ 0., 26.]]],\n",
       "\n",
       "\n",
       "          [[[ 0., 12.],\n",
       "            [ 0., 28.]],\n",
       "\n",
       "           [[ 0., 13.],\n",
       "            [ 0., 29.]],\n",
       "\n",
       "           [[ 0., 14.],\n",
       "            [ 0., 30.]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.,  5.],\n",
       "            [ 0., 21.]],\n",
       "\n",
       "           [[ 0.,  6.],\n",
       "            [ 0., 22.]],\n",
       "\n",
       "           [[ 0.,  7.],\n",
       "            [ 0., 23.]]],\n",
       "\n",
       "\n",
       "          [[[ 0.,  9.],\n",
       "            [ 0., 25.]],\n",
       "\n",
       "           [[ 0., 10.],\n",
       "            [ 0., 26.]],\n",
       "\n",
       "           [[ 0., 11.],\n",
       "            [ 0., 27.]]],\n",
       "\n",
       "\n",
       "          [[[ 0., 13.],\n",
       "            [ 0., 29.]],\n",
       "\n",
       "           [[ 0., 14.],\n",
       "            [ 0., 30.]],\n",
       "\n",
       "           [[ 0., 15.],\n",
       "            [ 0., 31.]]]]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_tr = patches_tr.transpose(0, 1, 2, 5, 6, 4, 3)\n",
    "patches_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,\n",
       "          4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,\n",
       "          8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.],\n",
       "        [ 1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,  0.,\n",
       "          5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,  0.,\n",
       "          9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,  0.],\n",
       "        [ 4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,\n",
       "          8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.,\n",
       "         12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0.],\n",
       "        [ 5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,  0.,\n",
       "          9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,  0.,\n",
       "         13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,  0., 31.,  0.],\n",
       "        [ 0.,  0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,\n",
       "          0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "          0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.],\n",
       "        [ 0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,\n",
       "          0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "          0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.],\n",
       "        [ 0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "          0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,\n",
       "          0., 12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,  0., 30.],\n",
       "        [ 0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "          0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,\n",
       "          0., 13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,  0., 31.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_rs = patches_tr.reshape(b, c_out*h_out*w_out, k*k*c_in*c_out)\n",
    "patches_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[[ 0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,\n",
       "              0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0.,\n",
       "             22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,\n",
       "              0., 26.,  0.],\n",
       "            [ 1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,\n",
       "              0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0.,\n",
       "             23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,\n",
       "              0., 27.,  0.],\n",
       "            [ 4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "              0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0.,\n",
       "             26.,  0., 12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,\n",
       "              0., 30.,  0.],\n",
       "            [ 5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "              0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0.,\n",
       "             27.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,\n",
       "              0., 31.,  0.],\n",
       "            [ 0.,  0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0.,\n",
       "             18.,  0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,\n",
       "              0., 22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0.,\n",
       "             10.,  0., 26.],\n",
       "            [ 0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0.,\n",
       "             19.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,\n",
       "              0., 23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.,\n",
       "             11.,  0., 27.],\n",
       "            [ 0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0.,\n",
       "             22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,\n",
       "              0., 26.,  0., 12.,  0., 28.,  0., 13.,  0., 29.,  0.,\n",
       "             14.,  0., 30.],\n",
       "            [ 0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0.,\n",
       "             23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,\n",
       "              0., 27.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0.,\n",
       "             15.,  0., 31.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,\n",
       "              0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0.,\n",
       "             22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,\n",
       "              0., 26.,  0.],\n",
       "            [ 1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,\n",
       "              0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0.,\n",
       "             23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,\n",
       "              0., 27.,  0.],\n",
       "            [ 4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "              0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0.,\n",
       "             26.,  0., 12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,\n",
       "              0., 30.,  0.],\n",
       "            [ 5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "              0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0.,\n",
       "             27.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,\n",
       "              0., 31.,  0.],\n",
       "            [ 0.,  0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0.,\n",
       "             18.,  0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,\n",
       "              0., 22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0.,\n",
       "             10.,  0., 26.],\n",
       "            [ 0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0.,\n",
       "             19.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,\n",
       "              0., 23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.,\n",
       "             11.,  0., 27.],\n",
       "            [ 0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0.,\n",
       "             22.,  0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,\n",
       "              0., 26.,  0., 12.,  0., 28.,  0., 13.,  0., 29.,  0.,\n",
       "             14.,  0., 30.],\n",
       "            [ 0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0.,\n",
       "             23.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,\n",
       "              0., 27.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0.,\n",
       "             15.,  0., 31.]]]]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(patches_rs, reps=(1, 2, 1, 1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0., 16.],\n",
       "          [ 1., 17.],\n",
       "          [ 2., 18.]],\n",
       "\n",
       "         [[ 4., 20.],\n",
       "          [ 5., 21.],\n",
       "          [ 6., 22.]],\n",
       "\n",
       "         [[ 8., 24.],\n",
       "          [ 9., 25.],\n",
       "          [10., 26.]]],\n",
       "\n",
       "\n",
       "        [[[ 1., 17.],\n",
       "          [ 2., 18.],\n",
       "          [ 3., 19.]],\n",
       "\n",
       "         [[ 5., 21.],\n",
       "          [ 6., 22.],\n",
       "          [ 7., 23.]],\n",
       "\n",
       "         [[ 9., 25.],\n",
       "          [10., 26.],\n",
       "          [11., 27.]]],\n",
       "\n",
       "\n",
       "        [[[ 4., 20.],\n",
       "          [ 5., 21.],\n",
       "          [ 6., 22.]],\n",
       "\n",
       "         [[ 8., 24.],\n",
       "          [ 9., 25.],\n",
       "          [10., 26.]],\n",
       "\n",
       "         [[12., 28.],\n",
       "          [13., 29.],\n",
       "          [14., 30.]]],\n",
       "\n",
       "\n",
       "        [[[ 5., 21.],\n",
       "          [ 6., 22.],\n",
       "          [ 7., 23.]],\n",
       "\n",
       "         [[ 9., 25.],\n",
       "          [10., 26.],\n",
       "          [11., 27.]],\n",
       "\n",
       "         [[13., 29.],\n",
       "          [14., 30.],\n",
       "          [15., 31.]]]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.transpose(0, 2, 3, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0., 16.],\n",
       "          [ 1., 17.],\n",
       "          [ 2., 18.]],\n",
       "\n",
       "         [[ 4., 20.],\n",
       "          [ 5., 21.],\n",
       "          [ 6., 22.]],\n",
       "\n",
       "         [[ 8., 24.],\n",
       "          [ 9., 25.],\n",
       "          [10., 26.]]],\n",
       "\n",
       "\n",
       "        [[[ 1., 17.],\n",
       "          [ 2., 18.],\n",
       "          [ 3., 19.]],\n",
       "\n",
       "         [[ 5., 21.],\n",
       "          [ 6., 22.],\n",
       "          [ 7., 23.]],\n",
       "\n",
       "         [[ 9., 25.],\n",
       "          [10., 26.],\n",
       "          [11., 27.]]],\n",
       "\n",
       "\n",
       "        [[[ 4., 20.],\n",
       "          [ 5., 21.],\n",
       "          [ 6., 22.]],\n",
       "\n",
       "         [[ 8., 24.],\n",
       "          [ 9., 25.],\n",
       "          [10., 26.]],\n",
       "\n",
       "         [[12., 28.],\n",
       "          [13., 29.],\n",
       "          [14., 30.]]],\n",
       "\n",
       "\n",
       "        [[[ 5., 21.],\n",
       "          [ 6., 22.],\n",
       "          [ 7., 23.]],\n",
       "\n",
       "         [[ 9., 25.],\n",
       "          [10., 26.],\n",
       "          [11., 27.]],\n",
       "\n",
       "         [[13., 29.],\n",
       "          [14., 30.],\n",
       "          [15., 31.]]]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_new = patches.transpose(0, 2, 3, 4, 1)\n",
    "patches_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,\n",
       "          4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,\n",
       "          8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.],\n",
       "        [ 1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,  0.,\n",
       "          5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,  0.,\n",
       "          9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,  0.],\n",
       "        [ 4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,\n",
       "          8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0.,\n",
       "         12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,  0., 30.,  0.],\n",
       "        [ 5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,  0.,\n",
       "          9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,  0.,\n",
       "         13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,  0., 31.,  0.],\n",
       "        [ 0.,  0.,  0., 16.,  0.,  1.,  0., 17.,  0.,  2.,  0., 18.,\n",
       "          0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "          0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.],\n",
       "        [ 0.,  1.,  0., 17.,  0.,  2.,  0., 18.,  0.,  3.,  0., 19.,\n",
       "          0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "          0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.],\n",
       "        [ 0.,  4.,  0., 20.,  0.,  5.,  0., 21.,  0.,  6.,  0., 22.,\n",
       "          0.,  8.,  0., 24.,  0.,  9.,  0., 25.,  0., 10.,  0., 26.,\n",
       "          0., 12.,  0., 28.,  0., 13.,  0., 29.,  0., 14.,  0., 30.],\n",
       "        [ 0.,  5.,  0., 21.,  0.,  6.,  0., 22.,  0.,  7.,  0., 23.,\n",
       "          0.,  9.,  0., 25.,  0., 10.,  0., 26.,  0., 11.,  0., 27.,\n",
       "          0., 13.,  0., 29.,  0., 14.,  0., 30.,  0., 15.,  0., 31.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = np.tile(np.eye(c_out), reps=(b, h_out*w_out, k, k, c_in, 1, 1))\n",
    "diag = np.einsum('bijklmm->bijklm', grad)\n",
    "diag[:] = patches_new[..., None]\n",
    "grad.transpose(0, 5, 1, 2, 3, 4, 6).reshape(b, c_out*h_out*w_out, k*k*c_in*c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding='same', stride=1, K_init=None, b_init=None):\n",
    "        # padding: 'same' или 'valid'\n",
    "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "        self.name = 'Conv2D'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel = K_init if K_init is not None \\\n",
    "                      else np.random.random((kernel_size, kernel_size, \n",
    "                                             input_channels, output_channels))\n",
    "        self.bias = b_init if b_init is not None \\\n",
    "                    else np.zeros(output_channels)\n",
    "        self.padding = padding        \n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "        # Нужно заполнить Numpy-тензор out\n",
    "\n",
    "        # Consistency check\n",
    "        self.check_input(input_data)\n",
    "\n",
    "        # Apply padding if needed\n",
    "        if self.padding == 'same':\n",
    "            input_data = self.pad(input_data, self.get_pads())\n",
    "\n",
    "        b, c_in, h_in, w_in = input_data.shape\n",
    "        c_out, h_out, w_out = self.output_channels, *self.get_output_shape((h_in, w_in))\n",
    "        \n",
    "        # Get patches\n",
    "        patches = self.get_patches(input_data, (h_out, w_out))\n",
    "\n",
    "        # Apply convolution w/ bias\n",
    "        conv = np.tensordot(patches, self.kernel, axes=([2, 3, 4], [0, 1, 2])) + self.bias\n",
    "\n",
    "        return conv.transpose(0, 2, 1).reshape(b, c_out, h_out, w_out)\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        # Consistency check\n",
    "        self.check_input(input_data)\n",
    "\n",
    "        # Apply padding if needed\n",
    "        if self.padding == 'same':\n",
    "            pad_l, pad_r = self.get_pads()  # used later\n",
    "            input_data = self.pad(input_data, (pad_l, pad_r))\n",
    "\n",
    "        k, s = self.kernel_size, self.stride\n",
    "        b, c_in, h_in, w_in = input_data.shape\n",
    "        c_out, h_out, w_out = self.output_channels, *self.get_output_shape((h_in, w_in))\n",
    "        \n",
    "        # Input height indexer\n",
    "        indh_in = np.arange(k) + np.expand_dims(s * np.arange(h_out), axis=1).repeat(k, axis=1)\n",
    "        indh_in = np.expand_dims(indh_in, axis=(1, 2, 4))\n",
    "\n",
    "        # Input width indexer\n",
    "        indw_in = np.arange(k) + np.expand_dims(s * np.arange(w_out), axis=1).repeat(k, axis=1)\n",
    "        indw_in = np.expand_dims(indw_in, axis=(0, 2, 3))\n",
    "\n",
    "        # Input channels indexer\n",
    "        indc_in = np.expand_dims(np.arange(c_in), axis=(0, 1, 3, 4))\n",
    "\n",
    "        # Output height/width indexers\n",
    "        indh_out = np.expand_dims(np.arange(h_out), axis=(1, 2, 3, 4))\n",
    "        indw_out = np.expand_dims(np.arange(w_out), axis=(0, 2, 3, 4))\n",
    "\n",
    "        # Prepare kernel\n",
    "        kernel = np.expand_dims(self.kernel.transpose(3, 2, 0, 1), axis=(0, 2, 3))\n",
    "\n",
    "        # Get the gradient in the form of feature maps\n",
    "        grad = np.zeros((b, c_out, h_out, w_out, c_in, h_in, w_in))\n",
    "        grad[..., indh_out, indw_out, indc_in, indh_in, indw_in] = kernel\n",
    "\n",
    "        # Remove padding if needed\n",
    "        if self.padding == 'same':\n",
    "            pad = pad_l + pad_r\n",
    "            h_in -= pad\n",
    "            w_in -= pad\n",
    "            grad = grad[..., pad_l:h_in+pad_l, pad_l:w_in+pad_l]\n",
    "\n",
    "        # Reshape to a batch of matrices\n",
    "        grad = grad.reshape(b, c_out * h_out * w_out, c_in * h_in * w_in)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def grad_kernel(self, input_data):\n",
    "        self.check_input(input_data)\n",
    "\n",
    "        # Apply padding if needed\n",
    "        if self.padding == 'same':\n",
    "            input_data = self.pad(input_data, self.get_pads())\n",
    "\n",
    "        k = self.kernel_size\n",
    "        b, c_in, h_in, w_in = input_data.shape\n",
    "        c_out, h_out, w_out = self.output_channels, *self.get_output_shape((h_in, w_in))\n",
    "        \n",
    "        # Get patches\n",
    "        patches = self.get_patches(input_data, (h_out, w_out))\n",
    "\n",
    "        # Get the gradient\n",
    "        grad = np.tile(np.eye(c_out), reps=(b, h_out*w_out, k, k, c_in, 1, 1))\n",
    "        diag = np.einsum('bijklmm->bijklm', grad)\n",
    "        diag[:] = patches[..., None]\n",
    "\n",
    "        return grad.transpose(0, 5, 1, 2, 3, 4, 6).reshape(b, c_out*h_out*w_out, k*k*c_in*c_out)\n",
    "\n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data)]\n",
    "\n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "\n",
    "    def check_input(self, input_data):\n",
    "        b, c_in, h_in, w_in = input_data.shape\n",
    "        if c_in != self.input_channels:\n",
    "            raise ValueError(f\"Input channels mismatch: \\\n",
    "                               got {c_in}, expected {self.input_channels}\")\n",
    "        if h_in < self.kernel_size or w_in < self.kernel_size:\n",
    "            raise ValueError(f\"Dimensions mismatch: \\\n",
    "                               got {h_in, w_in}, expected at least {self.kernel_size, self.kernel_size}\")\n",
    "\n",
    "    def get_output_shape(self, input_shape):\n",
    "        return (np.array(input_shape) - self.kernel_size) // self.stride + 1\n",
    "\n",
    "    def get_pads(self):\n",
    "        pad = self.kernel_size - 1\n",
    "        pad_l = pad // 2\n",
    "        pad_r = pad - pad_l\n",
    "        return pad_l, pad_r\n",
    "\n",
    "    def pad(self, data, pads):\n",
    "        return np.pad(data, ((0, 0), (0, 0), pads, pads))\n",
    "\n",
    "    def get_patches(self, input_data, output_shape):\n",
    "        k, s = self.kernel_size, self.stride\n",
    "        h_out, w_out = output_shape\n",
    "\n",
    "        # Height indexer\n",
    "        indh = np.arange(k) + np.expand_dims(s * np.arange(h_out), axis=1).repeat(k, axis=1)\n",
    "        indh = indh.repeat(w_out, axis=0)\n",
    "        indh = np.expand_dims(indh, axis=2)\n",
    "\n",
    "        # Width indexer\n",
    "        indw = np.arange(k) + np.expand_dims(s * np.arange(w_out), axis=1).repeat(k, axis=1)\n",
    "        indw = np.tile(indw, reps=(h_out, 1))\n",
    "        indw = np.expand_dims(indw, axis=1)\n",
    "\n",
    "        # Extract patches; keep batch and channels\n",
    "        patches = input_data[..., indh, indw]\n",
    "\n",
    "        return patches.transpose(0, 2, 3, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, c_in, c_out, h_in, w_in = 1, 2, 2, 4, 4\n",
    "input_data = np.arange(b*c_in*h_in*w_in).reshape((b, c_in, h_in, w_in)).astype('float32')\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, s = 3, 1\n",
    "kernel = np.tile(np.eye(k), reps=(c_in, 1, 1)) * np.array([[[1]], [[2]]])\n",
    "kernel = np.tile(kernel, reps=(c_out, 1, 1, 1))\n",
    "kernel = kernel.transpose(2, 3, 1, 0)\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[141., 150.],\n",
       "         [177., 186.]],\n",
       "\n",
       "        [[141., 150.],\n",
       "         [177., 186.]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Conv2DLayer(k, c_in, c_out, 'valid', s, kernel)\n",
    "layer.forward(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[142., 151.],\n",
       "         [178., 187.]],\n",
       "\n",
       "        [[143., 152.],\n",
       "         [179., 188.]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Conv2DLayer(k, c_in, c_out, 'valid', s, kernel, b_init=np.array([1, 2]))\n",
    "layer.forward(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DTrLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding=0, stride=1, K_init=None, b_init=None):      \n",
    "        # padding: число (сколько отрезать от модифицированной входной карты)\n",
    "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "        # stride - одно число (коэффициент расширения)\n",
    "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "        self.name = 'Conv2DTr'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel = K_init\n",
    "        self.bias = b_init\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "        # Нужно заполнить Numpy-тензор out \n",
    "        out = np.empty([])\n",
    "        return out\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    def grad_x(self):\n",
    "        pass\n",
    "    def grad_kernel(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Теперь настало время теста. \n",
    "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
    "\n",
    "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "np.random.seed(123)\n",
    " \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "print(keras.__version__)\n",
    "\n",
    "def get_keras_model():\n",
    "    input_image = Input(shape=(1, 28, 28))\n",
    "    flatten = Flatten()(input_image)\n",
    "    dense = Dense(10, activation='softmax')(flatten)\n",
    "    model = Model(inputs=input_image, outputs=dense)\n",
    "\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_split=0.25, \n",
    "                        batch_size=32, nb_epoch=2, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_our_model(keras_model):\n",
    "    W_keras, b_keras = keras_model.get_weights()\n",
    "    flatten = FlattenLayer()\n",
    "    dense = DenseLayer(784, 10, W_init=W_keras, b_init=b_keras)\n",
    "    softmax = Softmax()\n",
    "    return Network([flatten, dense, softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.4483 - acc: 0.8756 - val_loss: 0.3339 - val_acc: 0.9081\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.3253 - acc: 0.9076 - val_loss: 0.3074 - val_acc: 0.9143\n"
     ]
    }
   ],
   "source": [
    "keras_model = get_keras_model()\n",
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "keras_pred = keras_model.predict(X_test)\n",
    "our_pred = our_model.predict(X_test)\n",
    "if np.sum(np.abs(keras_pred - our_pred)) < 0.01:\n",
    "    print('Test PASSED')\n",
    "else:\n",
    "    print('Something went wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Вычисление производных по входу для слоёв нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании запрещено использовать численные формулы для вычисления производных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Реализуйте метод forward для класса CrossEntropy\n",
    "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
    "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'CrossEntropy'\n",
    "    \n",
    "    def forward(self, input_data, labels):\n",
    "        return -np.sum(labels * np.log(input_data), axis=1)\n",
    "    \n",
    "    def grad_x(self, input_data, labels):\n",
    "        return -labels / input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff_loss(loss, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (loss.forward(x + delta, labels) - loss.forward(x - delta, labels)) / (2 * eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_loss(loss):\n",
    "    x = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
    "    labels = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    \n",
    "    num_grad = numerical_diff_loss(loss, x, labels)\n",
    "    grad = loss.grad_x(x, labels)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropy()\n",
    "test_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff_layer(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x - delta)) / (2 * eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layer(layer):\n",
    "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
    "    \n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = Softmax()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Реализуйте метод grad_x для классов ReLU и DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(3, 4)\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def numerical_diff_conv(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i, j, k in itertools.product(*map(range, x[0].shape)):\n",
    "        delta = np.zeros(x[0].shape)\n",
    "        delta[i, j, k] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x - delta)) / (2 * eps)\n",
    "        right_answer.append(diff.reshape(len(x), -1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_conv(layer, b, c, h, w):\n",
    "    x = np.arange(b*c*h*w).reshape(b, c, h, w)\n",
    "    \n",
    "    num_grad = numerical_diff_conv(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "b, c_in, h_in, w_in, c_out = 5, 5, 5, 5, 5\n",
    "k, s = 3, 2\n",
    "conv = Conv2DLayer(k, c_in, c_out, 'same', s)\n",
    "test_conv(conv, b, c_in, h_in, w_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff_net(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x - delta, labels)) / (2 * eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_net(net):\n",
    "    x = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
    "    labels = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    \n",
    "    num_grad = numerical_diff_net(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_b(input_size, output_size, W, b, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(b)):\n",
    "        delta = np.zeros(b.shape)\n",
    "        delta[i] = eps\n",
    "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
    "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
    "        diff = (dense1.forward(x) - dense2.forward(x)) / (2 * eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_b():\n",
    "    input_size, output_size = 3, 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, W_init, b_init, x)\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "test_grad_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_W(input_size, output_size, W, b, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
    "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
    "            diff = (dense1.forward(x) - dense2.forward(x)) / (2 * eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_W():\n",
    "    input_size, output_size = 3, 4\n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((4,))\n",
    "    x = np.random.random((2, input_size))\n",
    "    \n",
    "    num_grad = numerical_grad_W(input_size, output_size, W_init, b_init, x)\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "test_grad_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def numerical_grad_kernel(kernel_size, input_channels, output_channels, \n",
    "                          padding, stride, K_init, b_init, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i, j, k, l in itertools.product(*map(range, K_init.shape)):\n",
    "        delta = np.zeros(K_init.shape)\n",
    "        delta[i, j, k, l] = eps\n",
    "        conv1 = Conv2DLayer(kernel_size, input_channels, output_channels, \n",
    "                            padding, stride, K_init+delta, b_init)\n",
    "        conv2 = Conv2DLayer(kernel_size, input_channels, output_channels, \n",
    "                            padding, stride, K_init-delta, b_init)\n",
    "        diff = (conv1.forward(x) - conv2.forward(x)) / (2 * eps)\n",
    "        right_answer.append(diff.reshape(len(x), -1).T)\n",
    "            \n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_kernel():\n",
    "    b, c_in, h_in, w_in, c_out = 5, 5, 5, 5, 5\n",
    "    k, s = 3, 2\n",
    "    \n",
    "    K_init = np.random.random((k, k, c_in, c_out))\n",
    "    b_init = np.random.random(c_out)\n",
    "\n",
    "    x = np.arange(b*c_in*h_in*w_in).reshape(b, c_in, h_in, w_in)\n",
    "    \n",
    "    num_grad = numerical_grad_kernel(k, c_in, c_out, 'same', s, K_init, b_init, x)\n",
    "    conv = Conv2DLayer(k, c_in, c_out, 'same', s, K_init, b_init)\n",
    "    grad = conv.grad_kernel(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "test_grad_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
    "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
    "\n",
    "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_param(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    W = net.layers[0].W\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            layer1 = DenseLayer(W.shape[0], W.shape[1], W_init=W+delta)\n",
    "            layer2 = DenseLayer(W.shape[0], W.shape[1], W_init=W-delta)\n",
    "            net1 = Network([layer1] + net.layers[1:], loss=net.loss)\n",
    "            net2 = Network([layer2] + net.layers[1:], loss=net.loss)\n",
    "            diff = (net1.calculate_loss(x, labels) - net2.calculate_loss(x, labels)) / (2 * eps)\n",
    "            right_answer.append(diff)\n",
    "    return np.array(right_answer).T.reshape(x.shape[0], W.shape[0], W.shape[1])\n",
    "\n",
    "def test_grad_param():\n",
    "    net = Network([DenseLayer(768, 20), ReLU(), DenseLayer(20, 3), Softmax()], loss=CrossEntropy())\n",
    "    x = np.random.random((2, 768))\n",
    "    labels = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "\n",
    "    num_grad = numerical_grad_param(net, x, labels)\n",
    "    grad = net.grad_param(x, labels)[0][0].reshape((x.shape[0],) + net.layers[0].W.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your grad is')\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "test_grad_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:08<00:00, 113.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:08<00:00, 114.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:08<00:00, 114.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:08<00:00, 114.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:08<00:00, 114.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.89\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::3], Y_train[::3], validation_split=0.25, \n",
    "        batch_size=16, nb_epoch=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:15<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:17<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:16<00:00, 29.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:15<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:15<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.65\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "        batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = [Conv2DLayer(input_channels=1, output_channels=1)]\n",
    "denses = [DenseLayer(784, 10), ReLU()]\n",
    "net = Network(convs + [FlattenLayer()] + denses + [Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[::6], Y_train[::6], validation_split=0.25, \n",
    "        batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:30<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:30<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:30<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:30<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:30<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.51\n"
     ]
    }
   ],
   "source": [
    "convs = [Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1),\n",
    "         Conv2DLayer(kernel_size=2, input_channels=1, output_channels=1, padding='valid', stride=2), \n",
    "         Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1), \n",
    "         Conv2DLayer(kernel_size=2, input_channels=1, output_channels=1, padding='valid', stride=2)]\n",
    "denses = [DenseLayer(49, 20), ReLU(), \n",
    "          DenseLayer(20, 10), ReLU()]\n",
    "net = Network(convs + [FlattenLayer()] + denses + [Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[::10], Y_train[::10], validation_split=0.25, \n",
    "        batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:53<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:52<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:52<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:53<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:53<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.54\n"
     ]
    }
   ],
   "source": [
    "convs = [Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1),\n",
    "         Conv2DLayer(kernel_size=2, input_channels=1, output_channels=1, padding='valid', stride=2), \n",
    "         Conv2DLayer(kernel_size=3, input_channels=1, output_channels=1, padding='same', stride=1), \n",
    "         Conv2DLayer(kernel_size=2, input_channels=1, output_channels=1, padding='valid', stride=2)]\n",
    "denses = [DenseLayer(49, 20), ReLU(),  \n",
    "          DenseLayer(20, 10), ReLU()]\n",
    "net = Network(convs + [FlattenLayer()] + denses + [Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[::6], Y_train[::6], validation_split=0.25, \n",
    "        batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
